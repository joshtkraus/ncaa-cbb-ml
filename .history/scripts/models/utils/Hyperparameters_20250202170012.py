def logistic_params(step):
    if step == 'Post':
        alpha = {
            2: [0.01, 0.05, 0.1, 0.5, 1],
            3: [0.01, 0.05, 0.1, 0.5, 1],
            4: [0.05, 0.1, 0.5, 1, 5],
            5: [0.05, 0.1, 0.5, 1, 5],
            6: [0.5, 1, 5, 10, 15],
            7: [1, 5, 10, 15, 20]
        }
    else:
        alpha = {
            2: [0.001, 0.01, 0.05, 0.1, 0.5],
            3: [0.001, 0.01, 0.05, 0.1, 0.5],
            4: [0.005, 0.01, 0.05, 0.1, 0.5],
            5: [0.005, 0.01, 0.05, 0.1, 0.5],
            6: [0.01, 0.05, 0.1, 0.5, 1],
            7: [0.05, 0.1, 0.5, 1, 5]
        }
    l1_ratio = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
    class_weight = ['balanced', None]
    penalty = ['elasticnet']
    return alpha, l1_ratio, class_weight, penalty

def random_forest(step):
    if step == 'Post':
        max_depth = {
            2: [5, 10, 15, 20],
            3: [15, 20, 25, 30],
            4: [20, 25, 30, 35],
            5: [20, 25, 30, 35],
            6: [5, 10, 15, 20],
            7: [10, 15, 20, 25]
        }
        n_estimators = {
            2: [10, 25, 50, 100],
            3: [100, 150, 200, 300],
            4: [150, 200, 300, 400],
            5: [50, 100, 150, 200],
            6: [50, 100, 150, 200],
            7: [50, 100, 150, 200]
        }
        min_samples_split = {
            2: [10, 15, 20, 25],
            3: [5, 10, 15, 20],
            4: [2, 3, 4, 5],
            5: [2, 3, 4, 5],
            6: [4, 5, 10, 15],
            7: [10, 15, 20, 25]
        }
    else:
        max_depth = {
            2: [None, 5, 10, 15, 20],
            3: [None, 10, 15, 20, 25],
            4: [None, 15, 20, 25, 30],
            5: [None, 15, 20, 25, 30],
            6: [None, 5, 10, 15, 20],
            7: [None, 10, 15, 20, 25]
        }
        n_estimators = {
            2: [50, 75, 100, 150],
            3: [150, 200, 250, 300],
            4: [200, 250, 300, 400],
            5: [100, 150, 200, 250],
            6: [100, 150, 200, 250],
            7: [100, 150, 200, 250]
        }
        min_samples_split = {
            2: [2, 5, 10, 15],
            3: [2, 5, 10, 15],
            4: [2, 3, 4, 5],
            5: [2, 3, 4, 5],
            6: [2, 4, 5, 10],
            7: [2, 5, 10, 15]
        }
    criterion = ['gini', 'entropy']
    class_weight = ['balanced', None]
    min_samples_leaf = [1, 2, 4]
    return max_depth, n_estimators, min_samples_split, criterion, class_weight, min_samples_leaf

def gradient_boosting(step):
    if step == 'Post':
        max_depth = {
            2: [1, 2, 4, 5],
            3: [10, 15, 20, 25],
            4: [15, 20, 25, 30],
            5: [10, 15, 20, 25],
            6: [5, 10, 15, 20],
            7: [2, 5, 10, 15]
        }
        n_estimators = {
            2: [50, 100, 150, 200],
            3: [10, 25, 50, 100],
            4: [10, 25, 50, 100],
            5: [10, 25, 50, 100],
            6: [25, 50, 100, 150],
            7: [100, 150, 200, 300]
        }
        min_samples_leaf = {
            2: [10, 20, 30, 40],
            3: [5, 10, 15, 20],
            4: [5, 10, 15, 20],
            5: [5, 10, 15, 20],
            6: [25, 50, 75, 100],
            7: [75, 100, 125, 150]
        }
    else:
        max_depth = {
            2: [1, 2, 3, 4, 5],
            3: [3, 5, 7, 10, 15],
            4: [5, 7, 10, 15, 20],
            5: [5, 7, 10, 15, 20],
            6: [3, 5, 7, 10, 15],
            7: [2, 4, 6, 8, 10]
        }
        n_estimators = {
            2: [50, 75, 100, 150],
            3: [75, 100, 150, 200],
            4: [100, 150, 200, 250],
            5: [100, 150, 200, 250],
            6: [75, 100, 150, 200],
            7: [100, 150, 200, 250]
        }
        min_samples_leaf = {
            2: [1, 5, 10, 20],
            3: [1, 5, 10, 15],
            4: [1, 5, 10, 15],
            5: [1, 5, 10, 15],
            6: [1, 5, 10, 20],
            7: [1, 5, 10, 15]
        }
    min_samples_split = [2,3,4]
    return max_depth, n_estimators, min_samples_leaf, min_samples_split

def neural_network(step):
    if step == 'Post':
        hidden_layer_size = {
            2: [(40, 20), (60, 30), (80, 40), (120, 60), (160, 80)],
            3: [(60,), (80,), (120,), (160,)],
            4: [(80, 40), (120, 60), (160, 80), (200, 100), (240, 120)],
            5: [(80, 40), (120, 60), (160, 80), (200, 100), (240, 120)],
            6: [(160,), (200,), (240,), (40, 20), (60, 30)],
            7: [(60, 30), (80, 40), (120, 60), (160, 80), (200, 100)]
        }
        alpha = {
            2: [0.5, 1, 5, 10, 15],
            3: [0.5, 1, 5, 10, 15],
            4: [0.001, 0.01, 0.1, 1],
            5: [0.001, 0.01, 0.1, 1],
            6: [0.0001, 0.001, 0.01, 0.1],
            7: [0.0001, 0.001, 0.01, 0.1]
        }
    else:
        hidden_layer_size = {
            2: [(40, 20), (60, 30), (80, 40)],
            3: [(60,), (80,), (120,)],
            4: [(80, 40), (120, 60), (160, 80)],
            5: [(80, 40), (120, 60), (160, 80)],
            6: [(160,), (200,), (240,)],
            7: [(60, 30), (80, 40), (120, 60)]
        }
        alpha = {
            2: [0.1, 0.5, 1, 5, 10],
            3: [0.1, 0.5, 1, 5, 10],
            4: [0.0001, 0.001, 0.01, 0.1],
            5: [0.0001, 0.001, 0.01, 0.1],
            6: [0.00001, 0.0001, 0.001, 0.01],
            7: [0.00001, 0.0001, 0.001, 0.01]
        }
    activation = ['tanh', 'relu']
    return hidden_layer_size, alpha, activation

def smote():
    k_neighbors = [1, 2, 3]
    return k_neighbors